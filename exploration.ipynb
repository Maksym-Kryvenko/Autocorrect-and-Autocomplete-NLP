{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrect and Autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use *auto-correct* everyday. When you send your friend a text message, or when you make a mistake in a query, there is an autocorrect behind the scenes that corrects the sentence for you. It based on minimum edit distance, which tells you the minimum amount of edits to change one word into another and also it involves dynamic programming methods which is an important programming concept and could be used to solve a lot of optimization problems.\n",
    "\n",
    "**Autocorrect:**\n",
    "\n",
    "<u>deah</u> -> dear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it works:\n",
    "1. Identify a misspelled word. (*deah*)\n",
    "2. Find string n edit distance away. (*_eah d_ar de_r* ... etc)\n",
    "3. Filter candidates. (*yeah dear dean* ... etc)\n",
    "4. Calculate probabilities. (how likely each word is to appear in this context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maks1\\AppData\\Local\\Temp\\ipykernel_2300\\376471819.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import re                       # regular expression library; for tokenization of words\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way to identify a word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check is a word in the vocabulary or no, if it isn't there it's misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word not in vocabulary:\n",
    "    misspelled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow ORANGE BLUE BLUE PINK\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "text = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # ðŸŒˆ\n",
    "print(text)\n",
    "print('string length : ',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow orange blue blue pink\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# convert all letters to lower case\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n",
    "print('string length : ',len(text_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\n",
      "count :  10\n"
     ]
    }
   ],
   "source": [
    "# some regex to tokenize the string to words and return them in a list\n",
    "words = re.findall(r'\\w+', text_lowercase)\n",
    "print(words)\n",
    "print('count : ',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red', 'pink', 'yellow', 'orange', 'blue'}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "vocab = set(words)\n",
    "print(vocab)\n",
    "print('count : ',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab including word count\n",
    "counts_a = dict()\n",
    "for w in words:\n",
    "    counts_a[w] = counts_a.get(w,0)+1\n",
    "print(counts_a)\n",
    "print('count : ',len(counts_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab including word count using collections.Counter\n",
    "counts_b = dict()\n",
    "counts_b = Counter(words)\n",
    "print(counts_b)\n",
    "print('count : ',len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barchart of sorted word counts\n",
    "d = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\n",
    "plt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n",
    "_ = plt.xticks(range(len(d)), list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to find a word:\n",
    "1. Insert (add a letter) *'to': 'two', 'top' ...*\n",
    "2. Delete (remove a letter) *'hat': 'ha', 'at', 'ht' ...*\n",
    "3. Switch (swap 2 adjacent letters) *'eta': 'eat', 'tea' ...*\n",
    "4. Replace (change 1 letter to another): *'jaw': 'jar', 'paw' ...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "word = 'dearz' # ðŸ¦Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'dearz']\n",
      "['d', 'earz']\n",
      "['de', 'arz']\n",
      "['dea', 'rz']\n",
      "['dear', 'z']\n",
      "['dearz', '']\n"
     ]
    }
   ],
   "source": [
    "# Find all the ways you can split a word into 2 parts !\n",
    "splits_a = []\n",
    "for i in range(len(word)+1):\n",
    "    splits_a.append([word[:i],word[i:]])\n",
    "\n",
    "for i in splits_a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "earz  <-- delete  d\n",
      "darz  <-- delete  e\n",
      "derz  <-- delete  a\n",
      "deaz  <-- delete  r\n",
      "dear  <-- delete  z\n"
     ]
    }
   ],
   "source": [
    "# Delete a letter from each string in the splits list.\n",
    "splits = splits_a\n",
    "deletes = []\n",
    "\n",
    "print('word : ', word)\n",
    "for L,R in splits:\n",
    "    if R:\n",
    "        print(L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "first item from the splits list :  ['', 'dearz']\n",
      "L :  \n",
      "R :  dearz\n",
      "*** now implicit delete by excluding the leading letter ***\n",
      "L + R[1:] :  earz  <-- delete  d\n"
     ]
    }
   ],
   "source": [
    "# It's worth taking a closer look at how this is excecuting a 'delete'.\n",
    "print('word : ', word)\n",
    "one_split = splits[0]\n",
    "print('first item from the splits list : ', one_split)\n",
    "L = one_split[0]\n",
    "R = one_split[1]\n",
    "print('L : ', L)\n",
    "print('R : ', R)\n",
    "print('*** now implicit delete by excluding the leading letter ***')\n",
    "print('L + R[1:] : ',L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "*** which is the same as ***\n",
      "earz\n",
      "darz\n",
      "derz\n",
      "deaz\n",
      "dear\n"
     ]
    }
   ],
   "source": [
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "\n",
    "print(deletes)\n",
    "print('*** which is the same as ***')\n",
    "for i in deletes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\n",
      "edits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "candidate words :  {'dear'}\n"
     ]
    }
   ],
   "source": [
    "# Next step will be to filter this list for candidate words found in a vocabulary.\n",
    "vocab = ['dean','deer','dear','fries','and','coke']\n",
    "edits = list(deletes)\n",
    "\n",
    "print('vocab : ', vocab)\n",
    "print('edits : ', edits)\n",
    "\n",
    "candidates=[]\n",
    "\n",
    "candidates = set.intersection(set(vocab), set(edits))\n",
    "\n",
    "print('candidate words : ', candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way to filter candidates:\n",
    "\n",
    "<u>deah</u> -----> <u>deah</u>\n",
    "\n",
    "_eah -----> yeah\n",
    "\n",
    "d_ar -----> dear\n",
    "\n",
    "de_r -----> dean\n",
    "\n",
    "...etc -----> ...etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum edit distance:\n",
    "\n",
    "Source: play -> Target: stay\n",
    "\n",
    "D[i,j] = source[:i] -> target[:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COST MATRIX\n",
    "# # # 0 1 2 3 4\n",
    "# # # # s t a y\n",
    "# 0 # 0 1 2 3 4\n",
    "# 1 p 1 2 3 4 5\n",
    "# 2 l 2 3 4 5 6\n",
    "# 3 a 3 4 5 4 5\n",
    "# 4 y 4 5 6 5 4\n",
    "\n",
    "# Once you get from p, l to s, d the suffix of both words is the same, a, y. \n",
    "# So there are no more edits needed. And that's why this 4 carries down the diagonal. \n",
    "\n",
    "# Source: play -> Target: stay\n",
    "# [0,0] -> (# -> #)\n",
    "# [1,0] -> (p -> #) delete\n",
    "# [0,1] -> (# -> s) insert\n",
    "# [1,1] -> (p -> s) \n",
    "#                  insert + delete: p -> ps -> s: 2\n",
    "#                  delete + insert: p ->  # -> s: 2\n",
    "#                  replace:         p -> s:       2\n",
    "\n",
    "# play -> #\n",
    "# D[i,j] = D[i-1,j] + delete_cost\n",
    "# D[4,0] = play -> #\n",
    "#        = source[:4] -> target[0]\n",
    "\n",
    "# # -> play\n",
    "# D[i,j] = D[i,j-1] + insert_cost\n",
    "\n",
    "#### GENERAL ####\n",
    "# D[i,j] = \n",
    "#\n",
    "#     Ð“ D[i-1,j] + delete_cost\n",
    "# min | D[i,j-1] + insert_cost\n",
    "#     L D[i-1,j-1] + Ð“ replace_cost; if source[i] != target[j]\n",
    "#                    L 0;            if source[i]  = target[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum edit distance\n",
    "* **Levenshtein distance.** Measuring the edit distance by using the three edits; insert, delete, and replace with costs 1, 1 and 2 respectively. That's what you've used here. \n",
    "* **Backtrace.** There are also well-known alternatives that have difference at its rules. Finding the minimum edit distance on its own doesn't always solve the whole problem. You sometimes need to know how you got there too. You do this by keeping a backtrace, which is simply a pointer in each cell letting you know where you came from to get there so you know the path taken across the table from the top left corner to the bottom right corner. This tells you the edits taken and is particularly useful in problems dealing with string alignment. \n",
    "* **Dynamic programming.** Finally, this tabular method for computation instead of brute force, is a technique known as dynamic programming. Intuitively, this just means that solving the smallest subproblem first and then reusing that result to solve the next biggest subproblem, saving that result, reusing it again and so on. This is what you did here by solving each cell in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way to calculate probabilities:\n",
    "\n",
    "Probability_of_a_word = (Numbers_of_time_the_word_appears) / (Total_size_of_the_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autocorrect', 'is', 'a', 'powerful', 'tool', 'and', 'it', 'is', 'used', 'on', 'our', 'computer.']\n",
      "P(is) =  0.17\n"
     ]
    }
   ],
   "source": [
    "senatance= 'Autocorrect is a powerful tool and it is used on our computer.'.split()\n",
    "print(senatance)\n",
    "print(\"P(is) = \", round(senatance.count('is') / len(senatance), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrect (full process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = []\n",
    "\n",
    "    # read file and convert into lower case\n",
    "    with open(file_name, 'r') as text_file:\n",
    "        raw_text = text_file.read().lower()\n",
    "    \n",
    "    # return as a list without special characters\n",
    "    words = re.findall('\\w+', raw_text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('D:\\Learning_py\\kaggle\\Autocorrect-and-Autocomplete-NLP\\input_data\\shakespeare.txt')\n",
    "vocab = set(word_l)\n",
    "print(word_l[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get count of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    word_count_dict = {}  # word counts\n",
    "    \n",
    "    for word in word_l:\n",
    "        word_count_dict[word] = word_count_dict.get(word,0) + 1        \n",
    "    \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = get_count(word_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {} \n",
    "\n",
    "    total = sum(word_count_dict.values())\n",
    "    \n",
    "    for key, val in word_count_dict.items():\n",
    "        probs[key] = val / total\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_probs(word_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_l = [''.join([word[:i], word[(i+1):]]) for i in range(len(word))]\n",
    "                                                   \n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return  delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word citrix, \n",
      "split_l = [('', 'citrix'), ('c', 'itrix'), ('ci', 'trix'), ('cit', 'rix'), ('citr', 'ix'), ('citri', 'x')], \n",
      "delete_l = ['itrix', 'ctrix', 'cirix', 'citix', 'citrx', 'citri']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"citrix\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    switch_l = [''.join([word[:i], word[i+1], word[i], word[(i+2):]]) for i in range(len(word)-1)]\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "    \n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = baeutiful \n",
      "split_l = [('', 'baeutiful'), ('b', 'aeutiful'), ('ba', 'eutiful'), ('bae', 'utiful'), ('baeu', 'tiful'), ('baeut', 'iful'), ('baeuti', 'ful'), ('baeutif', 'ul'), ('baeutifu', 'l')] \n",
      "switch_l = ['abeutiful', 'beautiful', 'bauetiful', 'baetuiful', 'baeuitful', 'baeutfiul', 'baeutiufl', 'baeutiflu']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"baeutiful\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    replace_set = {''.join([word[:word_i], letter, word[(word_i+1):]])\n",
    "                        for word_i in range(len(word))\n",
    "                        for letter in letters}\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set - {word}))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    insert_l = [''.join([word[:i], letter, word[i:]]) for i in range(len(word)+1) for letter in letters]\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word in \n",
      "split_l = [('', 'in'), ('i', 'n')] \n",
      "insert_l = ['ain', 'bin', 'cin', 'din', 'ein', 'fin', 'gin', 'hin', 'iin', 'jin', 'kin', 'lin', 'min', 'nin', 'oin', 'pin', 'qin', 'rin', 'sin', 'tin', 'uin', 'vin', 'win', 'xin', 'yin', 'zin', 'ian', 'ibn', 'icn', 'idn', 'ien', 'ifn', 'ign', 'ihn', 'iin', 'ijn', 'ikn', 'iln', 'imn', 'inn', 'ion', 'ipn', 'iqn', 'irn', 'isn', 'itn', 'iun', 'ivn', 'iwn', 'ixn', 'iyn', 'izn', 'ina', 'inb', 'inc', 'ind', 'ine', 'inf', 'ing', 'inh', 'ini', 'inj', 'ink', 'inl', 'inm', 'inn', 'ino', 'inp', 'inq', 'inr', 'ins', 'int', 'inu', 'inv', 'inw', 'inx', 'iny', 'inz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('in', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit one letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(set(delete_letter(word)))\n",
    "    edit_one_set.update(set(replace_letter(word)))\n",
    "    edit_one_set.update(set(insert_letter(word)))\n",
    "    \n",
    "    if allow_switches:\n",
    "        edit_one_set.update(set(switch_letter(word)))\n",
    "        \n",
    "    return set(edit_one_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word of \n",
      "edit_one_l \n",
      "['af', 'aof', 'bf', 'bof', 'cf', 'cof', 'df', 'dof', 'ef', 'eof', 'f', 'ff', 'fo', 'fof', 'gf', 'gof', 'hf', 'hof', 'if', 'iof', 'jf', 'jof', 'kf', 'kof', 'lf', 'lof', 'mf', 'mof', 'nf', 'nof', 'o', 'oa', 'oaf', 'ob', 'obf', 'oc', 'ocf', 'od', 'odf', 'oe', 'oef', 'ofa', 'ofb', 'ofc', 'ofd', 'ofe', 'off', 'ofg', 'ofh', 'ofi', 'ofj', 'ofk', 'ofl', 'ofm', 'ofn', 'ofo', 'ofp', 'ofq', 'ofr', 'ofs', 'oft', 'ofu', 'ofv', 'ofw', 'ofx', 'ofy', 'ofz', 'og', 'ogf', 'oh', 'ohf', 'oi', 'oif', 'oj', 'ojf', 'ok', 'okf', 'ol', 'olf', 'om', 'omf', 'on', 'onf', 'oo', 'oof', 'op', 'opf', 'oq', 'oqf', 'or', 'orf', 'os', 'osf', 'ot', 'otf', 'ou', 'ouf', 'ov', 'ovf', 'ow', 'owf', 'ox', 'oxf', 'oy', 'oyf', 'oz', 'ozf', 'pf', 'pof', 'qf', 'qof', 'rf', 'rof', 'sf', 'sof', 'tf', 'tof', 'uf', 'uof', 'vf', 'vof', 'wf', 'wof', 'xf', 'xof', 'yf', 'yof', 'zf', 'zof']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"of\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Two Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    \n",
    "    words = list(edit_one_letter(word, allow_switches)) \n",
    "    for word in words:\n",
    "        edit_two_set.update(edit_one_letter(word, allow_switches)) \n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggest Spelling Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    #Step 1: create suggestions as described above    \n",
    "    if word in vocab:\n",
    "        suggestions.append(word)\n",
    "    elif vocab and edit_one_letter(word, allow_switches=True):\n",
    "        suggestions.extend(edit_one_letter(word, allow_switches=True) & vocab)\n",
    "    else:\n",
    "        suggestions.extend(edit_two_letters(word, allow_switches=True) & vocab)\n",
    "    \n",
    "    suggestions = set(suggestions)\n",
    "    \n",
    "    #Step 2: determine probability of suggestions\n",
    "    sugg_prob = {i: probs[i] for i in suggestions}\n",
    "    \n",
    "    #Step 3: Get all your best words and return the most probable top n_suggested words as n_best\n",
    "    n_best = list(sorted(sugg_prob.items(), key=lambda x: x[1], reverse=True))[:n]\n",
    "      \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  loe \n",
      "suggestions =  {'lose', 'lov', 'love', 'le', 'lo', 'low', 'woe', 'lie'}\n",
      "word 0: love, probability 0.005204\n",
      "word 1: lose, probability 0.000541\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation \n",
    "my_word = 'loe' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "      \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0, col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]: # Replace None with a proper comparison\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min(\n",
    "                D[row - 1, col] + del_cost,  # Deletion\n",
    "                D[row, col - 1] + ins_cost,  # Insertion\n",
    "                D[row - 1, col - 1] + r_cost  # Replacement\n",
    "            )\n",
    "            \n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance with Backtrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edit distance isnâ€™t sufficient \n",
    "* * We often need to align each character of the two strings to each other \n",
    "* We do this by keeping a â€œbacktraceâ€ Every 'me we enter a cell, remember where we came from \n",
    "* When we reach the end, \n",
    "* * Trace back the path from the upper right corner to read off the alignment \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance_backtrace(source, target, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    \n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "    D = np.zeros((m + 1, n + 1), dtype=int)\n",
    "    operations = [[''] * (n + 1) for _ in range(m + 1)]  # Matrix to store operations\n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1, m + 1):\n",
    "        D[row, 0] = D[row - 1, 0] + del_cost\n",
    "        operations[row][0] = 'D'\n",
    "\n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1, n + 1):\n",
    "        D[0, col] = D[0, col - 1] + ins_cost\n",
    "        operations[0][col] = 'I'\n",
    "\n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1, m + 1):\n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1, n + 1):\n",
    "            r_cost = rep_cost if source[row - 1] != target[col - 1] else 0\n",
    "\n",
    "            if source[row - 1] == target[col - 1]:\n",
    "                r_cost = 0\n",
    "\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            deletion = D[row - 1, col] + del_cost\n",
    "            insertion = D[row, col - 1] + ins_cost\n",
    "            replacement = D[row - 1, col - 1] + r_cost\n",
    "\n",
    "            D[row, col] = min(deletion, insertion, replacement)\n",
    "\n",
    "            # Update operations matrix\n",
    "            if D[row, col] == deletion:\n",
    "                operations[row][col] = 'D'\n",
    "            elif D[row, col] == insertion:\n",
    "                operations[row][col] = 'I'\n",
    "            else:\n",
    "                operations[row][col] = 'R' if r_cost != 0 else 'M'\n",
    "\n",
    "    # Backtrace to get the sequence of operations\n",
    "    row, col = m, n\n",
    "    sequence = []\n",
    "\n",
    "    while row > 0 or col > 0:\n",
    "        if operations[row][col] == 'D':\n",
    "            sequence.append(f\"Delete '{source[row - 1]}' at position {row}\")\n",
    "            row -= 1\n",
    "        elif operations[row][col] == 'I':\n",
    "            sequence.append(f\"Insert '{target[col - 1]}' at position {col}\")\n",
    "            col -= 1\n",
    "        else:\n",
    "            if operations[row][col] == 'R':\n",
    "                if source[row - 1] != target[col - 1]:\n",
    "                    sequence.append(f\"Replace '{source[row - 1]}' with '{target[col - 1]}' at position {row}\")\n",
    "            else:\n",
    "                sequence.append(f\"Match '{source[row - 1]}' at position {row}\")\n",
    "\n",
    "            row -= 1\n",
    "            col -= 1\n",
    "\n",
    "    sequence.reverse()\n",
    "    med = D[m, n]\n",
    "\n",
    "    return D, sequence, med\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "[\"Insert 's' at position 1\", \"Insert 't' at position 2\", \"Delete 'p' at position 1\", \"Delete 'l' at position 2\", \"Match 'a' at position 3\", \"Match 'y' at position 4\"] \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, sequence, min_edits = min_edit_distance_backtrace(source, target)\n",
    "print(\"minimum edits: \", min_edits, \"\\n\")\n",
    "print(sequence,\"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocorrect_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
